
    \begin{conf-abstract}[]
        {\textbf{Acoustic Source Localization Model using  Audio-Visual Learning}}
        {\textit{Umadevi . F.M$^{1}$, Sujatha C$^{2}$, Abhishek saurabh$^{3}$}}
        {$^{1}$KLE Tech University $\bullet$ $^{2 ""}$KLETech, Hubballi"" $\bullet$ $^{3}$KLE technological University Hubballi}
        {\texttt{umadevifm@kletech.ac.in, sujatac@kletech.ac.in, abhisaur@gmail.com}}
        \indexauthors{F.M!Umadevi ., C!Sujatha, saurabh!Abhishek}
        {SoundSource Localization provides valuable information about  the spatial distribution of sound in our environment, enabling us to per ceive better, communicate, and interact with the world around us. It  provides crucial spatial information about the environment, allowing for  better perception and interaction with the world. In literature, multi ple techniques exist to extract features from an audio-visual input, such  as attention models and convolutional/deep neural networks. Many pre trained models are available for the task, such as VGG-19, I3D, I3D nonlocal, SlowFast, etc. In this paper, we summarize the recent advance ments in the domain, propose a methodology in which audio-visual input  is fed to the pre-trained C3D CNN model, and extract the video fea tures based on manually generated bounding boxes. The audio feature  extraction is done via the VGGish model. Both of these are then used  for training a fully connected neural network. Experimental results on  Audio-Visual Event Dataset and Flicker Soundnet Dataset have shown  satisfactory results. The performance metrics video accuracy with the  AVE dataset is 82.83\%, and the cIoU score is 0.75 on the AVE dataset.}
    \end{conf-abstract}
        