
    \begin{conf-abstract}[]
        {\textbf{GuideBP: Guided Backpropagation in multi-output neural networks by channeling gradients through weaker logits}}
        {\textit{Swarnendu Ghosh$^{1}$, Teresa Gon$^{Ã§alves2}$, Paulo Quaresma$^{3}$, Dr. Nibaran Das$^{4}$}}
        {$^{1}$Institute of Engineering \& Management $\bullet$ $^{2}$Universidade de vora $\bullet$ $^{3}$University of vora $\bullet$ $^{4}$Jadavpur University}
        {\texttt{drghosh90@gmail.com, tcg@uevora.pt;pq@uevora.pt, nibaran.das@jadavpuruniversity.in}}
        \indexauthors{Ghosh!Swarnendu, Gon!Teresa, Quaresma!Paulo, Das!Dr. Nibaran}
        {Convolutional neural networks often generate multiple logits from multiple networks. In most cases we use simple techniques like addition or column averaging for loss computation. But this allows gradients to be distributed equally among all paths. The proposed approach attempts to guide the gradients of backpropagation along weakest branches of the neural network. A weakness score is proposed that defines the class specific performance of individual logits. This is then used to create a new output distribution that would guide gradients along the weakest pathways. The proposed approach has been shown to perform better than traditional column merging techniques and can be used in several application scenarios. Not only can the proposed model be used as an efficient technique for training multiple instances of a model parallelly, but also CNNs with multiple output branches have been shown to perform better with the proposed upgrade. Various experiments establish the flexibility of the learning technique which is simple yet effective in various multi-objective scenarios both empirically and statistically.}
    \end{conf-abstract}
        